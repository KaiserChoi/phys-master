{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdee12bc-3cec-46d8-9c11-f351274c3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from utils import *\n",
    "from scipy.signal import savgol_filter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from models.timemil import *\n",
    "from models.inceptiontime import *\n",
    "from models.swin_transformer import *\n",
    "from torchviz import make_dot\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,precision_recall_fscore_support,f1_score,accuracy_score,precision_score,recall_score,balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13a5e42c-9f75-450b-9a92-864ffed7ae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiView(nn.Module):\n",
    "    def __init__(self,in_dim=137, out_dim=128, in_channels=64, out_channel=32, n_clip=3, clip_length=137, depth=2):\n",
    "        super(MultiView, self).__init__()\n",
    "        \n",
    "        self.n_clip = n_clip\n",
    "        self.ks = [3, 5, 17, 47]\n",
    "        self.channels = [32, 64, 128, 256]\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batches = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        \n",
    "        for i in range(depth): \n",
    "            self.convs.append(nn.Conv1d(1 if i==0 else self.channels[i-1], self.channels[i], self.ks[i], padding='same', stride=1))\n",
    "            self.batches.append(nn.BatchNorm1d(self.channels[i]))\n",
    "            self.activations.append(nn.ReLU())\n",
    "\n",
    "        # self.dropout = nn.Dropout(0.5)\n",
    "            \n",
    "            \n",
    "    def forward(self, x):\n",
    "        for conv, batch, activation in zip(self.convs, self.batches, self.activations):\n",
    "            x = conv(x)\n",
    "            x = batch(x)\n",
    "            x = activation(x)\n",
    "\n",
    "        return x[:,:,:-1]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60727309-117e-4eb3-8596-c5efd9bf9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobleView(nn.Module):\n",
    "    def __init__(self, in_dim=411, in_channels=64, out_dim=128, out_channel=32, depth=2):\n",
    "        super(GlobleView, self).__init__()\n",
    "        self.ks = [5, 17, 47]\n",
    "        self.channels = [32, 64, 128, 256]\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batches = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth): \n",
    "            self.convs.append(nn.Conv1d(1 if i==0 else self.channels[i-1], self.channels[i], self.ks[i], padding='same', stride=1))\n",
    "            self.batches.append(nn.BatchNorm1d(self.channels[i]))\n",
    "            self.activations.append(nn.ReLU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for conv, batch, activation in zip(self.convs, self.batches, self.activations):\n",
    "            x = activation(batch(conv(x)))\n",
    "        return x[:,:,:-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "394d9a8b-90de-4663-ad72-c969e86e0efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class V3(nn.Module):\n",
    "    def __init__(self, cross_in=64, in_dim=411, n_clip=3):\n",
    "        super(V3, self).__init__()\n",
    "        self.n_clip = n_clip\n",
    "        self.rand_index = torch.randperm(n_clip)\n",
    "        self.clip_length = in_dim // n_clip\n",
    "        \n",
    "        self.multiViews = nn.ModuleList()                       # [N,C,L] In: batch, channels, feature\n",
    "        self.g_view = GlobleView(out_dim=128, out_channel=32)   # [S,N,E] out: batch, channels, feature [N, S, E] if batch_firese\n",
    "\n",
    "        self.encodings = nn.ModuleList()\n",
    "        # self.g_encoding = TransformerEncoder(input_dim=411, embed_dim=128, num_heads=8, num_layers=1, dropout=0.1)      # seq embedding\n",
    "        # self.mv_encoder =TransformerEncoder(input_dim=137, embed_dim=128, num_heads=8, num_layers=1, dropout=0.1)\n",
    "        \n",
    "        self.g_encoding = SwinTransformer1D(num_classes=32, num_heads=[4, 8, 16, 32], patch_size=10, in_chans=64, embed_dim=64, drop_rate=0.0, drop_path_rate=0.5) # emberding=conv channels\n",
    "        self.mv_encoder = SwinTransformer1D(num_classes=32, num_heads=[4, 8, 16, 32], patch_size=8, in_chans=64, embed_dim=128, drop_rate=0.0, drop_path_rate=0.5)\n",
    "        \n",
    "        \n",
    "        self.crossAttentions = nn.ModuleList()\n",
    "        \n",
    "        for i in range(n_clip) : \n",
    "            self.multiViews.append(MultiView(out_dim=128, out_channel=32)) \n",
    "            self.crossAttentions.append(nn.MultiheadAttention(32, 8))\n",
    "\n",
    "        self._fc2 = nn.Sequential(\n",
    "            nn.Linear(32,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 4)) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        clips = torch.split(x, self.clip_length, dim=2)\n",
    "        x = self.g_view(x)                                             # (batch_size, channels, seq_len)\n",
    "        # print(x.shape)\n",
    "        # 使用 Swin Transformer 进行全局编码\n",
    "        x = self.g_encoding(x)    # (batch_size, channels, seq_len)\n",
    "        \n",
    "        for i in range(self.n_clip):\n",
    "            multiview = self.multiViews[i](clips[i])\n",
    "            \n",
    "            # print(multiview.shape)\n",
    "            \n",
    "            multiview = self.mv_encoder(multiview)  # Swin transformer block\n",
    "            attn_output, _ = self.crossAttentions[i](x, multiview, multiview)\n",
    "            outputs.append(attn_output)\n",
    "\n",
    "        x = self._fc2(x)\n",
    "        return x\n",
    "\n",
    "model = V3()\n",
    "model\n",
    "# 创建随机输入\n",
    "input_tensor = torch.randn(64, 1, 411)\n",
    "\n",
    "# 模型前向传播\n",
    "output = model(input_tensor)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972a5159-c18a-483a-86ee-876f013d9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_multiClass.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696408b5-6a70-41ec-aa69-a506f024f3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2566, 411)\n",
      "(2566,)\n"
     ]
    }
   ],
   "source": [
    "# 特征和标签\n",
    "X = data.drop('category', axis=1).values\n",
    "y = data['category'].values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9428a796-a76c-4ecd-ac61-697873c72435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1796, 1, 411)\n",
      "(1796,)\n",
      "(513, 1, 411)\n",
      "(513,)\n",
      "(257, 1, 411)\n",
      "(257,)\n"
     ]
    }
   ],
   "source": [
    "#标准化特征\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/3), random_state=42)\n",
    "\n",
    "# 将数据转换为卷积神经网络输入格式 (samples, timesteps, features)\n",
    "# 将数据转换为卷积神经网络输入格式 (samples, channels, sequence_length)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95261a4b-3630-44d5-b2d7-afab486c958c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1796, 1, 411])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor  = F.one_hot(torch.tensor(y_train)).float()\n",
    "y_val_tensor = F.one_hot(torch.tensor(y_val)).float()\n",
    "y_test_tensor = F.one_hot(torch.tensor(y_test)).float()\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9746c4e-4609-46e5-bd16-5261f5862481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_dataloader):\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, label) in enumerate(val_dataloader, 0):\n",
    "            # images = images.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.cuda()\n",
    "            label = label.cuda()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_labels.extend([label.cpu().numpy()])\n",
    "            val_predictions.extend([torch.sigmoid(outputs).cpu().numpy()])\n",
    "    \n",
    "    val_labels = np.vstack(val_labels)\n",
    "    val_predictions = np.vstack(val_predictions)\n",
    "\n",
    "    val_predictions_prob = np.exp(val_predictions)/np.sum(np.exp(val_predictions),axis=1,keepdims=True)\n",
    "    \n",
    "    \n",
    "    val_predictions = np.argmax(val_predictions,axis=1)\n",
    "    val_labels = np.argmax(val_labels,axis=1)\n",
    "    avg_score = accuracy_score(val_labels,val_predictions)\n",
    "    print('Accuracy of the network on the test dataset: {}'.format(avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c398955-0df9-475d-936f-b8b4d7c26771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, n_epochs=5):\n",
    "    best = 0\n",
    "    train_step = 0\n",
    "    losses = []\n",
    "\n",
    "    # 定义自定义日志目录名称\n",
    "    log_dir_name = \"V3_2_multiClass\"   # \"V3_multiClass\" 'V3_2_binary'\n",
    "    log_dir = f\"runs/{log_dir_name}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    # 初始化TensorBoard\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(f\"tensorboard --logdir={log_dir}\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        since = time.time()\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, targets = data\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            writer.add_scalar(\"train loss\", loss.item(), train_step)\n",
    "            train_step += 1\n",
    "        \n",
    "        epoch_duration = time.time() - since\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, duration: {epoch_duration:.2f}, loss: {epoch_loss:.6f}\")\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        # 验证模型\n",
    "        model.eval()\n",
    "        val_acc = val(model, val_loader)\n",
    "        writer.add_scalar(\"test acc\", val_acc, epoch + 1)\n",
    "        if best < val_acc:\n",
    "            best = val_acc\n",
    "            torch.save(model, \"V3_2_multiClass.pth\")\n",
    "\n",
    "        model.train()\n",
    "        #scheduler.step() # test_acc\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e4542b8-e5bc-4fb8-bf83-92e4e65c6928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 59634324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "V3(\n",
       "  (multiViews): ModuleList(\n",
       "    (0-2): 3 x MultiView(\n",
       "      (convs): ModuleList(\n",
       "        (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
       "        (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (batches): ModuleList(\n",
       "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (activations): ModuleList(\n",
       "        (0-1): 2 x ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (g_view): GlobleView(\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      (1): Conv1d(32, 64, kernel_size=(17,), stride=(1,), padding=same)\n",
       "    )\n",
       "    (batches): ModuleList(\n",
       "      (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (activations): ModuleList(\n",
       "      (0-1): 2 x ReLU()\n",
       "    )\n",
       "  )\n",
       "  (encodings): ModuleList()\n",
       "  (g_encoding): SwinTransformer1D(\n",
       "    (patch_embed): PatchEmbed1D(\n",
       "      (proj): Conv1d(64, 64, kernel_size=(10,), stride=(10,))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.045)\n",
       "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.136)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.182)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.227)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.273)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.318)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.364)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.409)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.455)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.500)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Sequential(\n",
       "      (0): AdaptiveAvgPool1d(output_size=1)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=512, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mv_encoder): SwinTransformer1D(\n",
       "    (patch_embed): PatchEmbed1D(\n",
       "      (proj): Conv1d(64, 128, kernel_size=(8,), stride=(8,))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.045)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.136)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.182)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.227)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.273)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.318)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.364)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.409)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.455)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock1D(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention1D(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.500)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (classifier): Sequential(\n",
       "      (0): AdaptiveAvgPool1d(output_size=1)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (crossAttentions): ModuleList(\n",
       "    (0-2): 3 x MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_fc2): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_ft = WaveLength(c_in=X.shape[1], c_out=6, nf=[47, 47, 47, 47])\n",
    "model_ft = V3().cuda()\n",
    "\n",
    "\n",
    "\n",
    "#lrscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_dataloader))\n",
    "# lrscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-9)\n",
    "\n",
    "# lrscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# parms_1x = [value for name, value in model.named_parameters()\n",
    "#             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "# parms_10x = [value for name, value in model.named_parameters()\n",
    "#             if name in [\"fc.weight\", \"fc.bias\"]]\n",
    "# optimizer = optim.AdamW([{\"params\": parms_1x},\n",
    "#                         {\"params\": parms_10x, 'lr': learning_rate * 10}], lr=learning_rate)\n",
    "\n",
    "\n",
    "learning_rate = 1e-5  # 2e-5\n",
    "epochs = 300  #200\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.AdamW(model_ft.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "# lrscheduler = optim.lr_scheduler.OneCycleLR(optimizer,max_lr=learning_rate,pct_start=0.5,total_steps=150,div_factor=10,final_div_factor=10)\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n",
    "# 打印模型的每一层参数量\n",
    "total_params = 0\n",
    "for name, param in model_ft.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # print(f\"{name}: {param.numel()}\")\n",
    "        total_params += param.numel()\n",
    "\n",
    "print(f\"Total number of trainable parameters: {total_params}\")\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b861d305-c2a4-4d80-ae79-e7041fdde31d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=runs/V3_2_multiClass_20240613-172153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, duration: 2.51, loss: 0.662008\n",
      "Accuracy of the network on the test dataset: 0.2573099415204678\n",
      "Epoch 2, duration: 2.34, loss: 0.604497\n",
      "Accuracy of the network on the test dataset: 0.28460038986354774\n",
      "Epoch 3, duration: 2.35, loss: 0.596603\n",
      "Accuracy of the network on the test dataset: 0.2787524366471735\n",
      "Epoch 4, duration: 2.33, loss: 0.594657\n",
      "Accuracy of the network on the test dataset: 0.30409356725146197\n",
      "Epoch 5, duration: 2.35, loss: 0.593662\n",
      "Accuracy of the network on the test dataset: 0.2884990253411306\n",
      "Epoch 6, duration: 2.33, loss: 0.589337\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 7, duration: 2.33, loss: 0.586234\n",
      "Accuracy of the network on the test dataset: 0.26705653021442494\n",
      "Epoch 8, duration: 2.39, loss: 0.586284\n",
      "Accuracy of the network on the test dataset: 0.30214424951267055\n",
      "Epoch 9, duration: 2.34, loss: 0.584529\n",
      "Accuracy of the network on the test dataset: 0.3001949317738791\n",
      "Epoch 10, duration: 2.35, loss: 0.579054\n",
      "Accuracy of the network on the test dataset: 0.2884990253411306\n",
      "Epoch 11, duration: 2.34, loss: 0.587706\n",
      "Accuracy of the network on the test dataset: 0.3079922027290448\n",
      "Epoch 12, duration: 2.37, loss: 0.584543\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 13, duration: 2.33, loss: 0.586766\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 14, duration: 2.35, loss: 0.582017\n",
      "Accuracy of the network on the test dataset: 0.2982456140350877\n",
      "Epoch 15, duration: 2.37, loss: 0.585464\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 16, duration: 2.36, loss: 0.582229\n",
      "Accuracy of the network on the test dataset: 0.2709551656920078\n",
      "Epoch 17, duration: 2.37, loss: 0.589681\n",
      "Accuracy of the network on the test dataset: 0.28654970760233917\n",
      "Epoch 18, duration: 2.36, loss: 0.582353\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 19, duration: 2.36, loss: 0.580511\n",
      "Accuracy of the network on the test dataset: 0.2982456140350877\n",
      "Epoch 20, duration: 2.37, loss: 0.582106\n",
      "Accuracy of the network on the test dataset: 0.29239766081871343\n",
      "Epoch 21, duration: 2.37, loss: 0.579625\n",
      "Accuracy of the network on the test dataset: 0.28460038986354774\n",
      "Epoch 22, duration: 2.38, loss: 0.583328\n",
      "Accuracy of the network on the test dataset: 0.290448343079922\n",
      "Epoch 23, duration: 2.38, loss: 0.582579\n",
      "Accuracy of the network on the test dataset: 0.2962962962962963\n",
      "Epoch 24, duration: 2.42, loss: 0.581679\n",
      "Accuracy of the network on the test dataset: 0.31189083820662766\n",
      "Epoch 25, duration: 2.42, loss: 0.577515\n",
      "Accuracy of the network on the test dataset: 0.29239766081871343\n",
      "Epoch 26, duration: 2.42, loss: 0.578979\n",
      "Accuracy of the network on the test dataset: 0.30214424951267055\n",
      "Epoch 27, duration: 2.66, loss: 0.578808\n",
      "Accuracy of the network on the test dataset: 0.30409356725146197\n",
      "Epoch 28, duration: 2.49, loss: 0.579642\n",
      "Accuracy of the network on the test dataset: 0.3001949317738791\n",
      "Epoch 29, duration: 2.43, loss: 0.577427\n",
      "Accuracy of the network on the test dataset: 0.2982456140350877\n",
      "Epoch 30, duration: 2.39, loss: 0.581108\n",
      "Accuracy of the network on the test dataset: 0.29239766081871343\n",
      "Epoch 31, duration: 2.40, loss: 0.579772\n",
      "Accuracy of the network on the test dataset: 0.27485380116959063\n",
      "Epoch 32, duration: 2.42, loss: 0.587391\n",
      "Accuracy of the network on the test dataset: 0.3157894736842105\n",
      "Epoch 33, duration: 2.52, loss: 0.584341\n",
      "Accuracy of the network on the test dataset: 0.3157894736842105\n",
      "Epoch 34, duration: 2.48, loss: 0.582860\n",
      "Accuracy of the network on the test dataset: 0.3079922027290448\n",
      "Epoch 35, duration: 2.41, loss: 0.579301\n",
      "Accuracy of the network on the test dataset: 0.3060428849902534\n",
      "Epoch 36, duration: 2.44, loss: 0.577092\n",
      "Accuracy of the network on the test dataset: 0.31189083820662766\n",
      "Epoch 37, duration: 2.42, loss: 0.584055\n",
      "Accuracy of the network on the test dataset: 0.3157894736842105\n",
      "Epoch 38, duration: 2.39, loss: 0.580139\n",
      "Accuracy of the network on the test dataset: 0.3138401559454191\n",
      "Epoch 39, duration: 2.39, loss: 0.574777\n",
      "Accuracy of the network on the test dataset: 0.29434697855750486\n",
      "Epoch 40, duration: 2.41, loss: 0.579640\n",
      "Accuracy of the network on the test dataset: 0.3001949317738791\n",
      "Epoch 41, duration: 2.40, loss: 0.579430\n",
      "Accuracy of the network on the test dataset: 0.3138401559454191\n",
      "Epoch 42, duration: 2.41, loss: 0.578980\n",
      "Accuracy of the network on the test dataset: 0.3235867446393762\n",
      "Epoch 43, duration: 2.44, loss: 0.575975\n",
      "Accuracy of the network on the test dataset: 0.3060428849902534\n",
      "Epoch 44, duration: 2.40, loss: 0.577230\n",
      "Accuracy of the network on the test dataset: 0.3216374269005848\n",
      "Epoch 45, duration: 2.39, loss: 0.582816\n",
      "Accuracy of the network on the test dataset: 0.30409356725146197\n",
      "Epoch 46, duration: 2.33, loss: 0.581313\n",
      "Accuracy of the network on the test dataset: 0.3216374269005848\n",
      "Epoch 47, duration: 2.33, loss: 0.576399\n",
      "Accuracy of the network on the test dataset: 0.3255360623781676\n",
      "Epoch 48, duration: 2.35, loss: 0.579112\n",
      "Accuracy of the network on the test dataset: 0.28654970760233917\n",
      "Epoch 49, duration: 2.33, loss: 0.577743\n",
      "Accuracy of the network on the test dataset: 0.34307992202729043\n",
      "Epoch 50, duration: 2.34, loss: 0.577289\n",
      "Accuracy of the network on the test dataset: 0.31968810916179335\n",
      "Epoch 51, duration: 2.33, loss: 0.573521\n",
      "Accuracy of the network on the test dataset: 0.3157894736842105\n",
      "Epoch 52, duration: 2.33, loss: 0.578785\n",
      "Accuracy of the network on the test dataset: 0.3060428849902534\n",
      "Epoch 53, duration: 2.35, loss: 0.576249\n",
      "Accuracy of the network on the test dataset: 0.3079922027290448\n",
      "Epoch 54, duration: 2.43, loss: 0.572764\n",
      "Accuracy of the network on the test dataset: 0.31773879142300193\n",
      "Epoch 55, duration: 2.35, loss: 0.575847\n",
      "Accuracy of the network on the test dataset: 0.3333333333333333\n",
      "Epoch 56, duration: 2.35, loss: 0.570855\n",
      "Accuracy of the network on the test dataset: 0.31773879142300193\n",
      "Epoch 57, duration: 2.36, loss: 0.575489\n",
      "Accuracy of the network on the test dataset: 0.3235867446393762\n",
      "Epoch 58, duration: 2.34, loss: 0.574921\n",
      "Accuracy of the network on the test dataset: 0.3313840155945419\n",
      "Epoch 59, duration: 2.34, loss: 0.579291\n",
      "Accuracy of the network on the test dataset: 0.32943469785575047\n",
      "Epoch 60, duration: 2.33, loss: 0.574236\n",
      "Accuracy of the network on the test dataset: 0.34502923976608185\n",
      "Epoch 61, duration: 2.37, loss: 0.569296\n",
      "Accuracy of the network on the test dataset: 0.3333333333333333\n",
      "Epoch 62, duration: 2.39, loss: 0.570089\n",
      "Accuracy of the network on the test dataset: 0.31968810916179335\n",
      "Epoch 63, duration: 2.44, loss: 0.572681\n",
      "Accuracy of the network on the test dataset: 0.3255360623781676\n",
      "Epoch 64, duration: 2.36, loss: 0.569728\n",
      "Accuracy of the network on the test dataset: 0.2884990253411306\n",
      "Epoch 65, duration: 2.35, loss: 0.574442\n",
      "Accuracy of the network on the test dataset: 0.3333333333333333\n",
      "Epoch 66, duration: 2.36, loss: 0.571991\n",
      "Accuracy of the network on the test dataset: 0.3255360623781676\n",
      "Epoch 67, duration: 2.53, loss: 0.570191\n",
      "Accuracy of the network on the test dataset: 0.31968810916179335\n",
      "Epoch 68, duration: 2.36, loss: 0.565828\n",
      "Accuracy of the network on the test dataset: 0.32748538011695905\n",
      "Epoch 69, duration: 2.38, loss: 0.569431\n",
      "Accuracy of the network on the test dataset: 0.33528265107212474\n",
      "Epoch 70, duration: 2.36, loss: 0.568375\n",
      "Accuracy of the network on the test dataset: 0.32943469785575047\n",
      "Epoch 71, duration: 2.36, loss: 0.565248\n",
      "Accuracy of the network on the test dataset: 0.35477582846003897\n",
      "Epoch 72, duration: 2.37, loss: 0.562794\n",
      "Accuracy of the network on the test dataset: 0.3157894736842105\n",
      "Epoch 73, duration: 2.34, loss: 0.564898\n",
      "Accuracy of the network on the test dataset: 0.3313840155945419\n",
      "Epoch 74, duration: 2.34, loss: 0.557208\n",
      "Accuracy of the network on the test dataset: 0.3216374269005848\n",
      "Epoch 75, duration: 2.34, loss: 0.562689\n",
      "Accuracy of the network on the test dataset: 0.3489278752436647\n",
      "Epoch 76, duration: 2.36, loss: 0.563218\n",
      "Accuracy of the network on the test dataset: 0.3469785575048733\n",
      "Epoch 77, duration: 2.36, loss: 0.560600\n",
      "Accuracy of the network on the test dataset: 0.33723196881091616\n",
      "Epoch 78, duration: 2.33, loss: 0.557020\n",
      "Accuracy of the network on the test dataset: 0.33723196881091616\n",
      "Epoch 79, duration: 2.33, loss: 0.560093\n",
      "Accuracy of the network on the test dataset: 0.32748538011695905\n",
      "Epoch 80, duration: 2.33, loss: 0.554951\n",
      "Accuracy of the network on the test dataset: 0.3586744639376218\n",
      "Epoch 81, duration: 2.50, loss: 0.568609\n",
      "Accuracy of the network on the test dataset: 0.3255360623781676\n",
      "Epoch 82, duration: 2.38, loss: 0.560316\n",
      "Accuracy of the network on the test dataset: 0.3469785575048733\n",
      "Epoch 83, duration: 2.37, loss: 0.557388\n",
      "Accuracy of the network on the test dataset: 0.36062378167641324\n",
      "Epoch 84, duration: 2.41, loss: 0.554416\n",
      "Accuracy of the network on the test dataset: 0.341130604288499\n",
      "Epoch 85, duration: 2.36, loss: 0.554458\n",
      "Accuracy of the network on the test dataset: 0.3235867446393762\n",
      "Epoch 86, duration: 2.43, loss: 0.563426\n",
      "Accuracy of the network on the test dataset: 0.34502923976608185\n",
      "Epoch 87, duration: 2.41, loss: 0.558769\n",
      "Accuracy of the network on the test dataset: 0.3762183235867446\n",
      "Epoch 88, duration: 2.41, loss: 0.552882\n",
      "Accuracy of the network on the test dataset: 0.37037037037037035\n",
      "Epoch 89, duration: 2.37, loss: 0.552448\n",
      "Accuracy of the network on the test dataset: 0.3664717348927875\n",
      "Epoch 90, duration: 2.39, loss: 0.552364\n",
      "Accuracy of the network on the test dataset: 0.3664717348927875\n",
      "Epoch 91, duration: 2.43, loss: 0.550727\n",
      "Accuracy of the network on the test dataset: 0.38011695906432746\n",
      "Epoch 92, duration: 2.33, loss: 0.547808\n",
      "Accuracy of the network on the test dataset: 0.38011695906432746\n",
      "Epoch 93, duration: 2.34, loss: 0.541379\n",
      "Accuracy of the network on the test dataset: 0.40350877192982454\n",
      "Epoch 94, duration: 2.35, loss: 0.544490\n",
      "Accuracy of the network on the test dataset: 0.4015594541910331\n",
      "Epoch 95, duration: 2.34, loss: 0.547879\n",
      "Accuracy of the network on the test dataset: 0.391812865497076\n",
      "Epoch 96, duration: 2.33, loss: 0.538662\n",
      "Accuracy of the network on the test dataset: 0.41325536062378165\n",
      "Epoch 97, duration: 2.35, loss: 0.534236\n",
      "Accuracy of the network on the test dataset: 0.43079922027290446\n",
      "Epoch 98, duration: 2.34, loss: 0.531908\n",
      "Accuracy of the network on the test dataset: 0.4522417153996101\n",
      "Epoch 99, duration: 2.34, loss: 0.526113\n",
      "Accuracy of the network on the test dataset: 0.4619883040935672\n",
      "Epoch 100, duration: 2.34, loss: 0.530189\n",
      "Accuracy of the network on the test dataset: 0.47953216374269003\n",
      "Epoch 101, duration: 2.35, loss: 0.521614\n",
      "Accuracy of the network on the test dataset: 0.4873294346978557\n",
      "Epoch 102, duration: 2.34, loss: 0.520580\n",
      "Accuracy of the network on the test dataset: 0.4600389863547758\n",
      "Epoch 103, duration: 2.35, loss: 0.516480\n",
      "Accuracy of the network on the test dataset: 0.4600389863547758\n",
      "Epoch 104, duration: 2.34, loss: 0.515018\n",
      "Accuracy of the network on the test dataset: 0.4600389863547758\n",
      "Epoch 105, duration: 2.33, loss: 0.521541\n",
      "Accuracy of the network on the test dataset: 0.4873294346978557\n",
      "Epoch 106, duration: 2.37, loss: 0.514730\n",
      "Accuracy of the network on the test dataset: 0.49122807017543857\n",
      "Epoch 107, duration: 2.39, loss: 0.512085\n",
      "Accuracy of the network on the test dataset: 0.49122807017543857\n",
      "Epoch 108, duration: 2.40, loss: 0.506251\n",
      "Accuracy of the network on the test dataset: 0.5165692007797271\n",
      "Epoch 109, duration: 2.40, loss: 0.503577\n",
      "Accuracy of the network on the test dataset: 0.5146198830409356\n",
      "Epoch 110, duration: 2.36, loss: 0.502558\n",
      "Accuracy of the network on the test dataset: 0.5087719298245614\n",
      "Epoch 111, duration: 2.39, loss: 0.499910\n",
      "Accuracy of the network on the test dataset: 0.530214424951267\n",
      "Epoch 112, duration: 2.40, loss: 0.500313\n",
      "Accuracy of the network on the test dataset: 0.530214424951267\n",
      "Epoch 113, duration: 2.37, loss: 0.498696\n",
      "Accuracy of the network on the test dataset: 0.5263157894736842\n",
      "Epoch 114, duration: 2.36, loss: 0.490083\n",
      "Accuracy of the network on the test dataset: 0.5165692007797271\n",
      "Epoch 115, duration: 2.36, loss: 0.501716\n",
      "Accuracy of the network on the test dataset: 0.5594541910331384\n",
      "Epoch 116, duration: 2.37, loss: 0.493451\n",
      "Accuracy of the network on the test dataset: 0.5867446393762183\n",
      "Epoch 117, duration: 2.39, loss: 0.489408\n",
      "Accuracy of the network on the test dataset: 0.5497076023391813\n",
      "Epoch 118, duration: 2.37, loss: 0.486505\n",
      "Accuracy of the network on the test dataset: 0.5828460038986355\n",
      "Epoch 119, duration: 2.36, loss: 0.487212\n",
      "Accuracy of the network on the test dataset: 0.5847953216374269\n",
      "Epoch 120, duration: 2.37, loss: 0.489302\n",
      "Accuracy of the network on the test dataset: 0.5769980506822612\n",
      "Epoch 121, duration: 2.37, loss: 0.491015\n",
      "Accuracy of the network on the test dataset: 0.6140350877192983\n",
      "Epoch 122, duration: 2.36, loss: 0.493831\n",
      "Accuracy of the network on the test dataset: 0.6081871345029239\n",
      "Epoch 123, duration: 2.36, loss: 0.474890\n",
      "Accuracy of the network on the test dataset: 0.5867446393762183\n",
      "Epoch 124, duration: 2.35, loss: 0.477212\n",
      "Accuracy of the network on the test dataset: 0.6276803118908382\n",
      "Epoch 125, duration: 2.38, loss: 0.462915\n",
      "Accuracy of the network on the test dataset: 0.6159844054580896\n",
      "Epoch 126, duration: 2.33, loss: 0.463270\n",
      "Accuracy of the network on the test dataset: 0.672514619883041\n",
      "Epoch 127, duration: 2.38, loss: 0.456290\n",
      "Accuracy of the network on the test dataset: 0.6179337231968811\n",
      "Epoch 128, duration: 2.33, loss: 0.460263\n",
      "Accuracy of the network on the test dataset: 0.6627680311890838\n",
      "Epoch 129, duration: 2.33, loss: 0.454886\n",
      "Accuracy of the network on the test dataset: 0.682261208576998\n",
      "Epoch 130, duration: 2.35, loss: 0.453271\n",
      "Accuracy of the network on the test dataset: 0.6705653021442495\n",
      "Epoch 131, duration: 2.35, loss: 0.446291\n",
      "Accuracy of the network on the test dataset: 0.6608187134502924\n",
      "Epoch 132, duration: 2.33, loss: 0.445353\n",
      "Accuracy of the network on the test dataset: 0.6023391812865497\n",
      "Epoch 133, duration: 2.35, loss: 0.448587\n",
      "Accuracy of the network on the test dataset: 0.695906432748538\n",
      "Epoch 134, duration: 2.43, loss: 0.453645\n",
      "Accuracy of the network on the test dataset: 0.7095516569200779\n",
      "Epoch 135, duration: 2.38, loss: 0.446754\n",
      "Accuracy of the network on the test dataset: 0.6998050682261209\n",
      "Epoch 136, duration: 2.35, loss: 0.445503\n",
      "Accuracy of the network on the test dataset: 0.6647173489278753\n",
      "Epoch 137, duration: 2.36, loss: 0.438719\n",
      "Accuracy of the network on the test dataset: 0.6803118908382066\n",
      "Epoch 138, duration: 2.36, loss: 0.421795\n",
      "Accuracy of the network on the test dataset: 0.7017543859649122\n",
      "Epoch 139, duration: 2.35, loss: 0.440795\n",
      "Accuracy of the network on the test dataset: 0.7037037037037037\n",
      "Epoch 140, duration: 2.37, loss: 0.431228\n",
      "Accuracy of the network on the test dataset: 0.7348927875243665\n",
      "Epoch 141, duration: 2.37, loss: 0.421310\n",
      "Accuracy of the network on the test dataset: 0.695906432748538\n",
      "Epoch 142, duration: 2.35, loss: 0.424036\n",
      "Accuracy of the network on the test dataset: 0.7037037037037037\n",
      "Epoch 143, duration: 2.36, loss: 0.415887\n",
      "Accuracy of the network on the test dataset: 0.7192982456140351\n",
      "Epoch 144, duration: 2.36, loss: 0.401918\n",
      "Accuracy of the network on the test dataset: 0.7543859649122807\n",
      "Epoch 145, duration: 2.38, loss: 0.414285\n",
      "Accuracy of the network on the test dataset: 0.7115009746588694\n",
      "Epoch 146, duration: 2.35, loss: 0.413305\n",
      "Accuracy of the network on the test dataset: 0.7348927875243665\n",
      "Epoch 147, duration: 2.36, loss: 0.404228\n",
      "Accuracy of the network on the test dataset: 0.7368421052631579\n",
      "Epoch 148, duration: 2.36, loss: 0.400011\n",
      "Accuracy of the network on the test dataset: 0.7699805068226121\n",
      "Epoch 149, duration: 2.38, loss: 0.400576\n",
      "Accuracy of the network on the test dataset: 0.7543859649122807\n",
      "Epoch 150, duration: 2.35, loss: 0.403320\n",
      "Accuracy of the network on the test dataset: 0.7543859649122807\n",
      "Epoch 151, duration: 2.34, loss: 0.405855\n",
      "Accuracy of the network on the test dataset: 0.732943469785575\n",
      "Epoch 152, duration: 2.33, loss: 0.395315\n",
      "Accuracy of the network on the test dataset: 0.746588693957115\n",
      "Epoch 153, duration: 2.33, loss: 0.392065\n",
      "Accuracy of the network on the test dataset: 0.7426900584795322\n",
      "Epoch 154, duration: 2.32, loss: 0.419211\n",
      "Accuracy of the network on the test dataset: 0.7251461988304093\n",
      "Epoch 155, duration: 2.33, loss: 0.391584\n",
      "Accuracy of the network on the test dataset: 0.7582846003898636\n",
      "Epoch 156, duration: 2.33, loss: 0.402713\n",
      "Accuracy of the network on the test dataset: 0.7563352826510721\n",
      "Epoch 157, duration: 2.36, loss: 0.386612\n",
      "Accuracy of the network on the test dataset: 0.7680311890838206\n",
      "Epoch 158, duration: 2.34, loss: 0.389065\n",
      "Accuracy of the network on the test dataset: 0.7719298245614035\n",
      "Epoch 159, duration: 2.36, loss: 0.400502\n",
      "Accuracy of the network on the test dataset: 0.7426900584795322\n",
      "Epoch 160, duration: 2.34, loss: 0.393189\n",
      "Accuracy of the network on the test dataset: 0.7582846003898636\n",
      "Epoch 161, duration: 2.37, loss: 0.365430\n",
      "Accuracy of the network on the test dataset: 0.783625730994152\n",
      "Epoch 162, duration: 2.37, loss: 0.372307\n",
      "Accuracy of the network on the test dataset: 0.7485380116959064\n",
      "Epoch 163, duration: 2.33, loss: 0.374706\n",
      "Accuracy of the network on the test dataset: 0.7777777777777778\n",
      "Epoch 164, duration: 2.36, loss: 0.369562\n",
      "Accuracy of the network on the test dataset: 0.7855750487329435\n",
      "Epoch 165, duration: 2.34, loss: 0.378706\n",
      "Accuracy of the network on the test dataset: 0.7719298245614035\n",
      "Epoch 166, duration: 2.35, loss: 0.360740\n",
      "Accuracy of the network on the test dataset: 0.783625730994152\n",
      "Epoch 167, duration: 2.34, loss: 0.355241\n",
      "Accuracy of the network on the test dataset: 0.8089668615984406\n",
      "Epoch 168, duration: 2.36, loss: 0.366264\n",
      "Accuracy of the network on the test dataset: 0.797270955165692\n",
      "Epoch 169, duration: 2.34, loss: 0.357577\n",
      "Accuracy of the network on the test dataset: 0.8050682261208577\n",
      "Epoch 170, duration: 2.35, loss: 0.370589\n",
      "Accuracy of the network on the test dataset: 0.7992202729044834\n",
      "Epoch 171, duration: 2.35, loss: 0.346497\n",
      "Accuracy of the network on the test dataset: 0.7816764132553606\n",
      "Epoch 172, duration: 2.35, loss: 0.338592\n",
      "Accuracy of the network on the test dataset: 0.8167641325536062\n",
      "Epoch 173, duration: 2.35, loss: 0.357044\n",
      "Accuracy of the network on the test dataset: 0.7758284600389863\n",
      "Epoch 174, duration: 2.34, loss: 0.339339\n",
      "Accuracy of the network on the test dataset: 0.7894736842105263\n",
      "Epoch 175, duration: 2.33, loss: 0.354347\n",
      "Accuracy of the network on the test dataset: 0.7504873294346979\n",
      "Epoch 176, duration: 2.33, loss: 0.365479\n",
      "Accuracy of the network on the test dataset: 0.7992202729044834\n",
      "Epoch 177, duration: 2.34, loss: 0.334590\n",
      "Accuracy of the network on the test dataset: 0.7933723196881092\n",
      "Epoch 178, duration: 2.33, loss: 0.342432\n",
      "Accuracy of the network on the test dataset: 0.8109161793372319\n",
      "Epoch 179, duration: 2.33, loss: 0.344761\n",
      "Accuracy of the network on the test dataset: 0.7855750487329435\n",
      "Epoch 180, duration: 2.37, loss: 0.335731\n",
      "Accuracy of the network on the test dataset: 0.8089668615984406\n",
      "Epoch 181, duration: 2.33, loss: 0.346112\n",
      "Accuracy of the network on the test dataset: 0.797270955165692\n",
      "Epoch 182, duration: 2.33, loss: 0.337242\n",
      "Accuracy of the network on the test dataset: 0.8128654970760234\n",
      "Epoch 183, duration: 2.39, loss: 0.318167\n",
      "Accuracy of the network on the test dataset: 0.7894736842105263\n",
      "Epoch 184, duration: 2.37, loss: 0.329853\n",
      "Accuracy of the network on the test dataset: 0.8206627680311891\n",
      "Epoch 185, duration: 2.42, loss: 0.340394\n",
      "Accuracy of the network on the test dataset: 0.7621832358674464\n",
      "Epoch 186, duration: 2.38, loss: 0.320468\n",
      "Accuracy of the network on the test dataset: 0.8011695906432749\n",
      "Epoch 187, duration: 2.36, loss: 0.338270\n",
      "Accuracy of the network on the test dataset: 0.8206627680311891\n",
      "Epoch 188, duration: 2.45, loss: 0.316734\n",
      "Accuracy of the network on the test dataset: 0.8226120857699805\n",
      "Epoch 189, duration: 2.38, loss: 0.322980\n",
      "Accuracy of the network on the test dataset: 0.8226120857699805\n",
      "Epoch 190, duration: 2.36, loss: 0.314410\n",
      "Accuracy of the network on the test dataset: 0.8206627680311891\n",
      "Epoch 191, duration: 2.40, loss: 0.325109\n",
      "Accuracy of the network on the test dataset: 0.7992202729044834\n",
      "Epoch 192, duration: 2.35, loss: 0.320295\n",
      "Accuracy of the network on the test dataset: 0.8128654970760234\n",
      "Epoch 193, duration: 2.39, loss: 0.310902\n",
      "Accuracy of the network on the test dataset: 0.7894736842105263\n",
      "Epoch 194, duration: 2.38, loss: 0.315351\n",
      "Accuracy of the network on the test dataset: 0.8167641325536062\n",
      "Epoch 195, duration: 2.36, loss: 0.308823\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 196, duration: 2.39, loss: 0.315401\n",
      "Accuracy of the network on the test dataset: 0.8050682261208577\n",
      "Epoch 197, duration: 2.38, loss: 0.305820\n",
      "Accuracy of the network on the test dataset: 0.8265107212475633\n",
      "Epoch 198, duration: 2.39, loss: 0.312002\n",
      "Accuracy of the network on the test dataset: 0.8148148148148148\n",
      "Epoch 199, duration: 2.40, loss: 0.300337\n",
      "Accuracy of the network on the test dataset: 0.834307992202729\n",
      "Epoch 200, duration: 2.40, loss: 0.297294\n",
      "Accuracy of the network on the test dataset: 0.8323586744639376\n",
      "Epoch 201, duration: 2.38, loss: 0.300593\n",
      "Accuracy of the network on the test dataset: 0.8265107212475633\n",
      "Epoch 202, duration: 2.38, loss: 0.308211\n",
      "Accuracy of the network on the test dataset: 0.8304093567251462\n",
      "Epoch 203, duration: 2.37, loss: 0.292717\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 204, duration: 2.36, loss: 0.299336\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 205, duration: 2.36, loss: 0.300832\n",
      "Accuracy of the network on the test dataset: 0.8401559454191033\n",
      "Epoch 206, duration: 2.36, loss: 0.306393\n",
      "Accuracy of the network on the test dataset: 0.8245614035087719\n",
      "Epoch 207, duration: 2.36, loss: 0.282751\n",
      "Accuracy of the network on the test dataset: 0.8284600389863548\n",
      "Epoch 208, duration: 2.35, loss: 0.294069\n",
      "Accuracy of the network on the test dataset: 0.8284600389863548\n",
      "Epoch 209, duration: 2.35, loss: 0.285615\n",
      "Accuracy of the network on the test dataset: 0.834307992202729\n",
      "Epoch 210, duration: 2.35, loss: 0.314371\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 211, duration: 2.35, loss: 0.285816\n",
      "Accuracy of the network on the test dataset: 0.8421052631578947\n",
      "Epoch 212, duration: 2.35, loss: 0.296583\n",
      "Accuracy of the network on the test dataset: 0.8304093567251462\n",
      "Epoch 213, duration: 2.35, loss: 0.280574\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 214, duration: 2.37, loss: 0.275059\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 215, duration: 2.41, loss: 0.284740\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 216, duration: 2.38, loss: 0.277912\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 217, duration: 2.37, loss: 0.272580\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 218, duration: 2.36, loss: 0.284039\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 219, duration: 2.36, loss: 0.279351\n",
      "Accuracy of the network on the test dataset: 0.8226120857699805\n",
      "Epoch 220, duration: 2.36, loss: 0.282285\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 221, duration: 2.36, loss: 0.285103\n",
      "Accuracy of the network on the test dataset: 0.8187134502923976\n",
      "Epoch 222, duration: 2.37, loss: 0.292453\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 223, duration: 2.36, loss: 0.280917\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 224, duration: 2.36, loss: 0.272117\n",
      "Accuracy of the network on the test dataset: 0.8265107212475633\n",
      "Epoch 225, duration: 2.36, loss: 0.271382\n",
      "Accuracy of the network on the test dataset: 0.8421052631578947\n",
      "Epoch 226, duration: 2.36, loss: 0.271619\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 227, duration: 2.34, loss: 0.270380\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 228, duration: 2.33, loss: 0.255135\n",
      "Accuracy of the network on the test dataset: 0.847953216374269\n",
      "Epoch 229, duration: 2.34, loss: 0.257920\n",
      "Accuracy of the network on the test dataset: 0.8187134502923976\n",
      "Epoch 230, duration: 2.33, loss: 0.263294\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 231, duration: 2.33, loss: 0.268846\n",
      "Accuracy of the network on the test dataset: 0.8362573099415205\n",
      "Epoch 232, duration: 2.33, loss: 0.253068\n",
      "Accuracy of the network on the test dataset: 0.8284600389863548\n",
      "Epoch 233, duration: 2.35, loss: 0.251010\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 234, duration: 2.34, loss: 0.242407\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 235, duration: 2.32, loss: 0.252052\n",
      "Accuracy of the network on the test dataset: 0.8401559454191033\n",
      "Epoch 236, duration: 2.33, loss: 0.253876\n",
      "Accuracy of the network on the test dataset: 0.8304093567251462\n",
      "Epoch 237, duration: 2.33, loss: 0.265244\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 238, duration: 2.38, loss: 0.251396\n",
      "Accuracy of the network on the test dataset: 0.8576998050682261\n",
      "Epoch 239, duration: 2.35, loss: 0.251479\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 240, duration: 2.32, loss: 0.249482\n",
      "Accuracy of the network on the test dataset: 0.8557504873294347\n",
      "Epoch 241, duration: 2.32, loss: 0.258743\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 242, duration: 2.34, loss: 0.254993\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 243, duration: 2.32, loss: 0.249478\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 244, duration: 2.32, loss: 0.227949\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 245, duration: 2.31, loss: 0.241739\n",
      "Accuracy of the network on the test dataset: 0.8615984405458089\n",
      "Epoch 246, duration: 2.31, loss: 0.238598\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 247, duration: 2.40, loss: 0.234889\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 248, duration: 2.37, loss: 0.247016\n",
      "Accuracy of the network on the test dataset: 0.8576998050682261\n",
      "Epoch 249, duration: 2.39, loss: 0.232771\n",
      "Accuracy of the network on the test dataset: 0.8421052631578947\n",
      "Epoch 250, duration: 2.35, loss: 0.235694\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 251, duration: 2.35, loss: 0.233076\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 252, duration: 2.33, loss: 0.238100\n",
      "Accuracy of the network on the test dataset: 0.8557504873294347\n",
      "Epoch 253, duration: 2.33, loss: 0.228043\n",
      "Accuracy of the network on the test dataset: 0.8323586744639376\n",
      "Epoch 254, duration: 2.34, loss: 0.232000\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 255, duration: 2.32, loss: 0.220585\n",
      "Accuracy of the network on the test dataset: 0.8596491228070176\n",
      "Epoch 256, duration: 2.32, loss: 0.231637\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 257, duration: 2.32, loss: 0.256290\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 258, duration: 2.32, loss: 0.234825\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 259, duration: 2.32, loss: 0.227945\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 260, duration: 2.32, loss: 0.223330\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 261, duration: 2.32, loss: 0.228054\n",
      "Accuracy of the network on the test dataset: 0.8265107212475633\n",
      "Epoch 262, duration: 2.31, loss: 0.250954\n",
      "Accuracy of the network on the test dataset: 0.8265107212475633\n",
      "Epoch 263, duration: 2.31, loss: 0.283303\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 264, duration: 2.31, loss: 0.274455\n",
      "Accuracy of the network on the test dataset: 0.8401559454191033\n",
      "Epoch 265, duration: 2.30, loss: 0.231064\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 266, duration: 2.30, loss: 0.225309\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 267, duration: 2.34, loss: 0.224260\n",
      "Accuracy of the network on the test dataset: 0.8732943469785575\n",
      "Epoch 268, duration: 2.33, loss: 0.224987\n",
      "Accuracy of the network on the test dataset: 0.8596491228070176\n",
      "Epoch 269, duration: 2.32, loss: 0.278352\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 270, duration: 2.32, loss: 0.260217\n",
      "Accuracy of the network on the test dataset: 0.834307992202729\n",
      "Epoch 271, duration: 2.37, loss: 0.245212\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 272, duration: 2.33, loss: 0.223575\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 273, duration: 2.32, loss: 0.216603\n",
      "Accuracy of the network on the test dataset: 0.8362573099415205\n",
      "Epoch 274, duration: 2.32, loss: 0.222519\n",
      "Accuracy of the network on the test dataset: 0.8499025341130604\n",
      "Epoch 275, duration: 2.35, loss: 0.210395\n",
      "Accuracy of the network on the test dataset: 0.8693957115009746\n",
      "Epoch 276, duration: 2.33, loss: 0.201079\n",
      "Accuracy of the network on the test dataset: 0.8596491228070176\n",
      "Epoch 277, duration: 2.33, loss: 0.214054\n",
      "Accuracy of the network on the test dataset: 0.8576998050682261\n",
      "Epoch 278, duration: 2.33, loss: 0.206149\n",
      "Accuracy of the network on the test dataset: 0.847953216374269\n",
      "Epoch 279, duration: 2.34, loss: 0.220555\n",
      "Accuracy of the network on the test dataset: 0.8693957115009746\n",
      "Epoch 280, duration: 2.33, loss: 0.210973\n",
      "Accuracy of the network on the test dataset: 0.8615984405458089\n",
      "Epoch 281, duration: 2.33, loss: 0.196366\n",
      "Accuracy of the network on the test dataset: 0.8635477582846004\n",
      "Epoch 282, duration: 2.33, loss: 0.196409\n",
      "Accuracy of the network on the test dataset: 0.8693957115009746\n",
      "Epoch 283, duration: 2.32, loss: 0.218076\n",
      "Accuracy of the network on the test dataset: 0.8732943469785575\n",
      "Epoch 284, duration: 2.33, loss: 0.237592\n",
      "Accuracy of the network on the test dataset: 0.8596491228070176\n",
      "Epoch 285, duration: 2.32, loss: 0.217818\n",
      "Accuracy of the network on the test dataset: 0.8557504873294347\n",
      "Epoch 286, duration: 2.32, loss: 0.265660\n",
      "Accuracy of the network on the test dataset: 0.834307992202729\n",
      "Epoch 287, duration: 2.33, loss: 0.250662\n",
      "Accuracy of the network on the test dataset: 0.8401559454191033\n",
      "Epoch 288, duration: 2.32, loss: 0.222349\n",
      "Accuracy of the network on the test dataset: 0.8382066276803118\n",
      "Epoch 289, duration: 2.32, loss: 0.227756\n",
      "Accuracy of the network on the test dataset: 0.8401559454191033\n",
      "Epoch 290, duration: 2.32, loss: 0.203146\n",
      "Accuracy of the network on the test dataset: 0.8421052631578947\n",
      "Epoch 291, duration: 2.33, loss: 0.220463\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 292, duration: 2.31, loss: 0.190836\n",
      "Accuracy of the network on the test dataset: 0.847953216374269\n",
      "Epoch 293, duration: 2.31, loss: 0.204644\n",
      "Accuracy of the network on the test dataset: 0.8557504873294347\n",
      "Epoch 294, duration: 2.31, loss: 0.202489\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 295, duration: 2.31, loss: 0.214202\n",
      "Accuracy of the network on the test dataset: 0.8557504873294347\n",
      "Epoch 296, duration: 2.33, loss: 0.259743\n",
      "Accuracy of the network on the test dataset: 0.8050682261208577\n",
      "Epoch 297, duration: 2.32, loss: 0.253837\n",
      "Accuracy of the network on the test dataset: 0.8245614035087719\n",
      "Epoch 298, duration: 2.32, loss: 0.218135\n",
      "Accuracy of the network on the test dataset: 0.847953216374269\n",
      "Epoch 299, duration: 2.32, loss: 0.206666\n",
      "Accuracy of the network on the test dataset: 0.8362573099415205\n",
      "Epoch 300, duration: 2.33, loss: 0.201630\n",
      "Accuracy of the network on the test dataset: 0.8362573099415205\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model_ft, training_losses = train(model_ft, criterion, optimizer, lrscheduler, n_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "953915ed-6ec4-4536-b8b0-3ea93cbe6126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('V3_2_multiClass.pth')\n",
    "model.cuda()\n",
    "model.eval()  # 切换模型到评估模式z\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aea65a73-69e6-4552-a571-241ef5f686da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8794\n",
      "f1_marco: 0.8804\n",
      "p_marco: 0.8817\n",
      "r_marco: 0.8820\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, label) in enumerate(test_loader, 0):\n",
    "        # images = images.to(device).half() # uncomment for half precision model\n",
    "        inputs = inputs.cuda()\n",
    "        label = label.cuda()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        test_labels.extend([label.cpu().numpy()])\n",
    "        test_predictions.extend([torch.sigmoid(outputs).cpu().numpy()])\n",
    "\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "test_predictions_prob = np.exp(test_predictions)/np.sum(np.exp(test_predictions),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "test_predictions = np.argmax(test_predictions,axis=1)\n",
    "test_labels = np.argmax(test_labels,axis=1)\n",
    "\n",
    "avg_score = accuracy_score(test_labels, test_predictions)\n",
    "p_marco = precision_score(test_labels,test_predictions,average='macro')\n",
    "r_marco = recall_score(test_labels,test_predictions,average='macro')\n",
    "f1_marco = f1_score(test_labels,test_predictions,average='macro')\n",
    "print(\"Average accuracy: {:.4f}\".format(avg_score))\n",
    "print(\"f1_marco: {:.4f}\".format(f1_marco))\n",
    "print(\"p_marco: {:.4f}\".format(p_marco))\n",
    "print(\"r_marco: {:.4f}\".format(r_marco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2075ef-ddd3-4001-a112-ce53fd7304b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ 1 1 2 2 3 3 4 4 5 5 0 0]\n",
    "# [1 0 2 2 3 5 4 5 5 5 0 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
