{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef2073b-597f-45fd-8b1d-7a20ff810f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import time\n",
    "#from model import InceptionTimePlus\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,precision_recall_fscore_support,f1_score,accuracy_score,precision_score,recall_score,balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38aadd25-dfa4-4a3f-9017-71edbd762e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>640</th>\n",
       "      <th>641</th>\n",
       "      <th>642</th>\n",
       "      <th>643</th>\n",
       "      <th>644</th>\n",
       "      <th>645</th>\n",
       "      <th>646</th>\n",
       "      <th>647</th>\n",
       "      <th>648</th>\n",
       "      <th>649</th>\n",
       "      <th>...</th>\n",
       "      <th>1042</th>\n",
       "      <th>1043</th>\n",
       "      <th>1044</th>\n",
       "      <th>1045</th>\n",
       "      <th>1046</th>\n",
       "      <th>1047</th>\n",
       "      <th>1048</th>\n",
       "      <th>1049</th>\n",
       "      <th>1050</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.126331</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.126215</td>\n",
       "      <td>0.126215</td>\n",
       "      <td>0.126040</td>\n",
       "      <td>0.125808</td>\n",
       "      <td>0.125634</td>\n",
       "      <td>0.125692</td>\n",
       "      <td>0.125866</td>\n",
       "      <td>0.125808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149905</td>\n",
       "      <td>0.149783</td>\n",
       "      <td>0.149538</td>\n",
       "      <td>0.149231</td>\n",
       "      <td>0.148803</td>\n",
       "      <td>0.148253</td>\n",
       "      <td>0.147764</td>\n",
       "      <td>0.147642</td>\n",
       "      <td>0.147947</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142547</td>\n",
       "      <td>0.142366</td>\n",
       "      <td>0.142245</td>\n",
       "      <td>0.142185</td>\n",
       "      <td>0.142065</td>\n",
       "      <td>0.141824</td>\n",
       "      <td>0.141403</td>\n",
       "      <td>0.141042</td>\n",
       "      <td>0.140742</td>\n",
       "      <td>0.140561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177309</td>\n",
       "      <td>0.177113</td>\n",
       "      <td>0.176330</td>\n",
       "      <td>0.175094</td>\n",
       "      <td>0.174055</td>\n",
       "      <td>0.173472</td>\n",
       "      <td>0.173277</td>\n",
       "      <td>0.172825</td>\n",
       "      <td>0.171920</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067932</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>0.068135</td>\n",
       "      <td>0.068186</td>\n",
       "      <td>0.068186</td>\n",
       "      <td>0.068288</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>0.068593</td>\n",
       "      <td>0.068898</td>\n",
       "      <td>0.069357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127844</td>\n",
       "      <td>0.127494</td>\n",
       "      <td>0.127669</td>\n",
       "      <td>0.127494</td>\n",
       "      <td>0.126040</td>\n",
       "      <td>0.125228</td>\n",
       "      <td>0.125344</td>\n",
       "      <td>0.124707</td>\n",
       "      <td>0.123955</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033483</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.033248</td>\n",
       "      <td>0.033201</td>\n",
       "      <td>0.033108</td>\n",
       "      <td>0.032920</td>\n",
       "      <td>0.032733</td>\n",
       "      <td>0.032592</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.032686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085551</td>\n",
       "      <td>0.086557</td>\n",
       "      <td>0.086557</td>\n",
       "      <td>0.085287</td>\n",
       "      <td>0.083757</td>\n",
       "      <td>0.083073</td>\n",
       "      <td>0.083336</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>0.083651</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.082967</td>\n",
       "      <td>0.083073</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.082915</td>\n",
       "      <td>0.082810</td>\n",
       "      <td>0.082862</td>\n",
       "      <td>0.083125</td>\n",
       "      <td>0.083493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142969</td>\n",
       "      <td>0.142366</td>\n",
       "      <td>0.142607</td>\n",
       "      <td>0.142547</td>\n",
       "      <td>0.141102</td>\n",
       "      <td>0.140381</td>\n",
       "      <td>0.140621</td>\n",
       "      <td>0.139542</td>\n",
       "      <td>0.138764</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>0.187421</td>\n",
       "      <td>0.186953</td>\n",
       "      <td>0.186419</td>\n",
       "      <td>0.185952</td>\n",
       "      <td>0.185553</td>\n",
       "      <td>0.185153</td>\n",
       "      <td>0.184821</td>\n",
       "      <td>0.184489</td>\n",
       "      <td>0.184090</td>\n",
       "      <td>0.183825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093557</td>\n",
       "      <td>0.093396</td>\n",
       "      <td>0.093503</td>\n",
       "      <td>0.093611</td>\n",
       "      <td>0.093611</td>\n",
       "      <td>0.092965</td>\n",
       "      <td>0.091890</td>\n",
       "      <td>0.091461</td>\n",
       "      <td>0.091301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>0.223226</td>\n",
       "      <td>0.222863</td>\n",
       "      <td>0.222501</td>\n",
       "      <td>0.222138</td>\n",
       "      <td>0.221776</td>\n",
       "      <td>0.221415</td>\n",
       "      <td>0.221126</td>\n",
       "      <td>0.220764</td>\n",
       "      <td>0.220404</td>\n",
       "      <td>0.220187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138048</td>\n",
       "      <td>0.138346</td>\n",
       "      <td>0.138585</td>\n",
       "      <td>0.137690</td>\n",
       "      <td>0.135608</td>\n",
       "      <td>0.134896</td>\n",
       "      <td>0.135726</td>\n",
       "      <td>0.135548</td>\n",
       "      <td>0.134837</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>0.216168</td>\n",
       "      <td>0.215954</td>\n",
       "      <td>0.215597</td>\n",
       "      <td>0.215169</td>\n",
       "      <td>0.214670</td>\n",
       "      <td>0.214243</td>\n",
       "      <td>0.213888</td>\n",
       "      <td>0.213604</td>\n",
       "      <td>0.213391</td>\n",
       "      <td>0.213249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137988</td>\n",
       "      <td>0.136499</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.135964</td>\n",
       "      <td>0.134955</td>\n",
       "      <td>0.134541</td>\n",
       "      <td>0.135015</td>\n",
       "      <td>0.135074</td>\n",
       "      <td>0.134363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>0.246340</td>\n",
       "      <td>0.245958</td>\n",
       "      <td>0.245422</td>\n",
       "      <td>0.244888</td>\n",
       "      <td>0.244430</td>\n",
       "      <td>0.244201</td>\n",
       "      <td>0.244049</td>\n",
       "      <td>0.243744</td>\n",
       "      <td>0.243288</td>\n",
       "      <td>0.242832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173277</td>\n",
       "      <td>0.171340</td>\n",
       "      <td>0.171405</td>\n",
       "      <td>0.171920</td>\n",
       "      <td>0.170632</td>\n",
       "      <td>0.169925</td>\n",
       "      <td>0.170632</td>\n",
       "      <td>0.170761</td>\n",
       "      <td>0.169861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.053695</td>\n",
       "      <td>0.053548</td>\n",
       "      <td>0.053449</td>\n",
       "      <td>0.053351</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.052959</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>0.052664</td>\n",
       "      <td>0.052566</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.031144</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>0.030444</td>\n",
       "      <td>0.031144</td>\n",
       "      <td>0.031050</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           640       641       642       643       644       645       646  \\\n",
       "0     0.126331  0.126273  0.126215  0.126215  0.126040  0.125808  0.125634   \n",
       "1     0.142547  0.142366  0.142245  0.142185  0.142065  0.141824  0.141403   \n",
       "2     0.067932  0.068034  0.068135  0.068186  0.068186  0.068288  0.068390   \n",
       "3     0.033483  0.033342  0.033248  0.033201  0.033108  0.032920  0.032733   \n",
       "4     0.082862  0.082862  0.082967  0.083073  0.083020  0.082915  0.082810   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2561  0.187421  0.186953  0.186419  0.185952  0.185553  0.185153  0.184821   \n",
       "2562  0.223226  0.222863  0.222501  0.222138  0.221776  0.221415  0.221126   \n",
       "2563  0.216168  0.215954  0.215597  0.215169  0.214670  0.214243  0.213888   \n",
       "2564  0.246340  0.245958  0.245422  0.244888  0.244430  0.244201  0.244049   \n",
       "2565  0.053695  0.053548  0.053449  0.053351  0.053204  0.052959  0.052762   \n",
       "\n",
       "           647       648       649  ...      1042      1043      1044  \\\n",
       "0     0.125692  0.125866  0.125808  ...  0.149905  0.149783  0.149538   \n",
       "1     0.141042  0.140742  0.140561  ...  0.177309  0.177113  0.176330   \n",
       "2     0.068593  0.068898  0.069357  ...  0.127844  0.127494  0.127669   \n",
       "3     0.032592  0.032546  0.032686  ...  0.085551  0.086557  0.086557   \n",
       "4     0.082862  0.083125  0.083493  ...  0.142969  0.142366  0.142607   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2561  0.184489  0.184090  0.183825  ...  0.093557  0.093396  0.093503   \n",
       "2562  0.220764  0.220404  0.220187  ...  0.138048  0.138346  0.138585   \n",
       "2563  0.213604  0.213391  0.213249  ...  0.137988  0.136499  0.136083   \n",
       "2564  0.243744  0.243288  0.242832  ...  0.173277  0.171340  0.171405   \n",
       "2565  0.052664  0.052566  0.052419  ...  0.031984  0.030537  0.030631   \n",
       "\n",
       "          1045      1046      1047      1048      1049      1050  category  \n",
       "0     0.149231  0.148803  0.148253  0.147764  0.147642  0.147947         3  \n",
       "1     0.175094  0.174055  0.173472  0.173277  0.172825  0.171920         3  \n",
       "2     0.127494  0.126040  0.125228  0.125344  0.124707  0.123955         3  \n",
       "3     0.085287  0.083757  0.083073  0.083336  0.083599  0.083651         3  \n",
       "4     0.142547  0.141102  0.140381  0.140621  0.139542  0.138764         3  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2561  0.093611  0.093611  0.092965  0.091890  0.091461  0.091301         0  \n",
       "2562  0.137690  0.135608  0.134896  0.135726  0.135548  0.134837         0  \n",
       "2563  0.135964  0.134955  0.134541  0.135015  0.135074  0.134363         0  \n",
       "2564  0.171920  0.170632  0.169925  0.170632  0.170761  0.169861         0  \n",
       "2565  0.031144  0.030537  0.030444  0.031144  0.031050  0.030305         0  \n",
       "\n",
       "[2566 rows x 412 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_multiClass.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6259c0-3d5b-457e-a00e-91a8478eaca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2566, 411)\n",
      "(2566,)\n"
     ]
    }
   ],
   "source": [
    "# 特征和标签\n",
    "X = data.drop('category', axis=1).values\n",
    "y = data['category'].values\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edf11e3-28b4-473a-8bdc-481a40beeb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1539, 1, 411)\n",
      "(1539,)\n",
      "(513, 1, 411)\n",
      "(513,)\n",
      "(514, 1, 411)\n",
      "(514,)\n"
     ]
    }
   ],
   "source": [
    "#标准化特征\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/2), random_state=42)\n",
    "\n",
    "# 将数据转换为卷积神经网络输入格式 (samples, timesteps, features)\n",
    "# 将数据转换为卷积神经网络输入格式 (samples, channels, sequence_length)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51dbd38-c2e5-4f00-b405-30f1b542cda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1539, 1, 411])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将数据转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor  = F.one_hot(torch.tensor(y_train)).float()\n",
    "y_val_tensor = F.one_hot(torch.tensor(y_val)).float()\n",
    "y_test_tensor = F.one_hot(torch.tensor(y_test)).float()\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b141656c-136f-49a3-89bd-2a8f527e4a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN1D(\n",
       "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(17,), stride=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(47,), stride=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=2944, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            # m.bias.data.zero_()\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "# 定义卷积神经网络模型\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_length, num_class=4):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=17)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=47)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self._get_conv_output_size(input_length), 128)\n",
    "        self.fc2 = nn.Linear(128, num_class)\n",
    "\n",
    "        initialize_weights(self)\n",
    "        \n",
    "    def _get_conv_output_size(self, input_length):\n",
    "        size = input_length\n",
    "        size = (size - 4) // 2  # conv1 and pool1\n",
    "        size = (size - 16) // 2  # conv2 and pool2\n",
    "        size = (size - 46) // 2  # conv3 and pool3\n",
    "        return size * 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "CNN1D(411)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321340ea-c432-4e72-bb49-a8356c28544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, n_epochs=5):\n",
    "    best = 0\n",
    "    train_step = 0\n",
    "    losses = []\n",
    "\n",
    "    # 定义自定义日志目录名称\n",
    "    log_dir_name = \"V1_6_multiClass\"   # \"V3_multiClass\" 'V3_2_binary'\n",
    "    log_dir = f\"runs/{log_dir_name}_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    # 初始化TensorBoard\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(f\"tensorboard --logdir={log_dir}\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        since = time.time()\n",
    "        train_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, targets = data\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            writer.add_scalar(\"train loss\", loss.item(), train_step)\n",
    "            train_step += 1\n",
    "        \n",
    "        epoch_duration = time.time() - since\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, duration: {epoch_duration:.2f}, loss: {epoch_loss:.6f}\")\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "        # 验证模型\n",
    "        model.eval()\n",
    "        val_acc = val(model, val_loader)\n",
    "        writer.add_scalar(\"val acc\", val_acc, epoch + 1)\n",
    "        if best < val_acc:\n",
    "            best = val_acc\n",
    "            torch.save(model, \"weights/V1_6_multiClass.pth\")\n",
    "\n",
    "        model.train()\n",
    "        #scheduler.step() # test_acc\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987ae8c3-ae53-4367-8277-a196cbedba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, val_dataloader):\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, label) in enumerate(val_dataloader, 0):\n",
    "            # images = images.to(device).half() # uncomment for half precision model\n",
    "            inputs = inputs.cuda()\n",
    "            label = label.cuda()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            val_labels.extend([label.cpu().numpy()])\n",
    "            val_predictions.extend([torch.sigmoid(outputs).cpu().numpy()])\n",
    "    \n",
    "    val_labels = np.vstack(val_labels)\n",
    "    val_predictions = np.vstack(val_predictions)\n",
    "\n",
    "    val_predictions_prob = np.exp(val_predictions)/np.sum(np.exp(val_predictions),axis=1,keepdims=True)\n",
    "    \n",
    "    \n",
    "    val_predictions = np.argmax(val_predictions,axis=1)\n",
    "    val_labels = np.argmax(val_labels,axis=1)\n",
    "    avg_score = accuracy_score(val_labels,val_predictions)\n",
    "    print('Accuracy of the network on the test dataset: {}'.format(avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2d0c3f-b1b4-438a-9e13-3d4e818a6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 798148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN1D(\n",
       "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,))\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(32, 64, kernel_size=(17,), stride=(1,))\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv1d(64, 128, kernel_size=(47,), stride=(1,))\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=2944, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义OneCycleLR学习率调整策略\n",
    "learning_rate = 5e-5\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "\n",
    "# model_ft = WaveLength(c_in=X.shape[1], c_out=6, nf=[47, 47, 47, 47])\n",
    "model_ft = CNN1D(411, 4).cuda()\n",
    "criterion = nn.CrossEntropyLoss() # nn.BCEWithLogitsLoss()\n",
    "\n",
    "#lrscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_dataloader))\n",
    "# lrscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=20, eta_min=1e-9)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=learning_rate, weight_decay=learning_rate*0.1)\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=17, verbose=True)\n",
    "# parms_1x = [value for name, value in model.named_parameters()\n",
    "#             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "# parms_10x = [value for name, value in model.named_parameters()\n",
    "#             if name in [\"fc.weight\", \"fc.bias\"]]\n",
    "# optimizer = optim.Adam([{\"params\": parms_1x},\n",
    "#                         {\"params\": parms_10x, 'lr': learning_rate * 10}], lr=learning_rate)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_ft.parameters())\n",
    "print(f'Total parameters: {total_params}')  # 798406 参数大概合理\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1831226-ac87-42cd-939f-0fc956bd6063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=runs/V1_6_multiClass_20240612-025226\n",
      "Epoch 1, duration: 0.32, loss: 1.374939\n",
      "Accuracy of the network on the test dataset: 0.26900584795321636\n",
      "Epoch 2, duration: 0.32, loss: 1.265772\n",
      "Accuracy of the network on the test dataset: 0.31773879142300193\n",
      "Epoch 3, duration: 0.32, loss: 1.161101\n",
      "Accuracy of the network on the test dataset: 0.3684210526315789\n",
      "Epoch 4, duration: 0.32, loss: 1.084077\n",
      "Accuracy of the network on the test dataset: 0.43664717348927873\n",
      "Epoch 5, duration: 0.32, loss: 0.986192\n",
      "Accuracy of the network on the test dataset: 0.4600389863547758\n",
      "Epoch 6, duration: 0.32, loss: 0.965795\n",
      "Accuracy of the network on the test dataset: 0.5048732943469786\n",
      "Epoch 7, duration: 0.32, loss: 0.942783\n",
      "Accuracy of the network on the test dataset: 0.5458089668615984\n",
      "Epoch 8, duration: 0.32, loss: 0.861615\n",
      "Accuracy of the network on the test dataset: 0.5886939571150097\n",
      "Epoch 9, duration: 0.32, loss: 0.839662\n",
      "Accuracy of the network on the test dataset: 0.6140350877192983\n",
      "Epoch 10, duration: 0.32, loss: 0.804100\n",
      "Accuracy of the network on the test dataset: 0.6237816764132553\n",
      "Epoch 11, duration: 0.32, loss: 0.786142\n",
      "Accuracy of the network on the test dataset: 0.6062378167641326\n",
      "Epoch 12, duration: 0.32, loss: 0.737435\n",
      "Accuracy of the network on the test dataset: 0.6237816764132553\n",
      "Epoch 13, duration: 0.32, loss: 0.685860\n",
      "Accuracy of the network on the test dataset: 0.6101364522417154\n",
      "Epoch 14, duration: 0.32, loss: 0.677418\n",
      "Accuracy of the network on the test dataset: 0.6842105263157895\n",
      "Epoch 15, duration: 0.32, loss: 0.669843\n",
      "Accuracy of the network on the test dataset: 0.6666666666666666\n",
      "Epoch 16, duration: 0.32, loss: 0.666600\n",
      "Accuracy of the network on the test dataset: 0.6783625730994152\n",
      "Epoch 17, duration: 0.32, loss: 0.626974\n",
      "Accuracy of the network on the test dataset: 0.6588693957115009\n",
      "Epoch 18, duration: 0.32, loss: 0.678929\n",
      "Accuracy of the network on the test dataset: 0.6530214424951267\n",
      "Epoch 19, duration: 0.32, loss: 0.575152\n",
      "Accuracy of the network on the test dataset: 0.7309941520467836\n",
      "Epoch 20, duration: 0.32, loss: 0.518410\n",
      "Accuracy of the network on the test dataset: 0.7485380116959064\n",
      "Epoch 21, duration: 0.32, loss: 0.561683\n",
      "Accuracy of the network on the test dataset: 0.7387914230019493\n",
      "Epoch 22, duration: 0.32, loss: 0.500014\n",
      "Accuracy of the network on the test dataset: 0.723196881091618\n",
      "Epoch 23, duration: 0.32, loss: 0.461187\n",
      "Accuracy of the network on the test dataset: 0.746588693957115\n",
      "Epoch 24, duration: 0.32, loss: 0.439203\n",
      "Accuracy of the network on the test dataset: 0.7563352826510721\n",
      "Epoch 25, duration: 0.32, loss: 0.453869\n",
      "Accuracy of the network on the test dataset: 0.7855750487329435\n",
      "Epoch 26, duration: 0.32, loss: 0.469769\n",
      "Accuracy of the network on the test dataset: 0.7407407407407407\n",
      "Epoch 27, duration: 0.32, loss: 0.466885\n",
      "Accuracy of the network on the test dataset: 0.7153996101364523\n",
      "Epoch 28, duration: 0.32, loss: 0.455365\n",
      "Accuracy of the network on the test dataset: 0.7719298245614035\n",
      "Epoch 29, duration: 0.32, loss: 0.454356\n",
      "Accuracy of the network on the test dataset: 0.7524366471734892\n",
      "Epoch 30, duration: 0.32, loss: 0.585221\n",
      "Accuracy of the network on the test dataset: 0.6900584795321637\n",
      "Epoch 31, duration: 0.32, loss: 0.544517\n",
      "Accuracy of the network on the test dataset: 0.7212475633528265\n",
      "Epoch 32, duration: 0.32, loss: 0.496354\n",
      "Accuracy of the network on the test dataset: 0.7504873294346979\n",
      "Epoch 33, duration: 0.32, loss: 0.454225\n",
      "Accuracy of the network on the test dataset: 0.7777777777777778\n",
      "Epoch 34, duration: 0.32, loss: 0.385279\n",
      "Accuracy of the network on the test dataset: 0.7992202729044834\n",
      "Epoch 35, duration: 0.32, loss: 0.353734\n",
      "Accuracy of the network on the test dataset: 0.7894736842105263\n",
      "Epoch 36, duration: 0.32, loss: 0.323028\n",
      "Accuracy of the network on the test dataset: 0.7992202729044834\n",
      "Epoch 37, duration: 0.32, loss: 0.290938\n",
      "Accuracy of the network on the test dataset: 0.8284600389863548\n",
      "Epoch 38, duration: 0.32, loss: 0.313584\n",
      "Accuracy of the network on the test dataset: 0.8187134502923976\n",
      "Epoch 39, duration: 0.32, loss: 0.288266\n",
      "Accuracy of the network on the test dataset: 0.847953216374269\n",
      "Epoch 40, duration: 0.32, loss: 0.254791\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 41, duration: 0.32, loss: 0.217850\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 42, duration: 0.32, loss: 0.202450\n",
      "Accuracy of the network on the test dataset: 0.8538011695906432\n",
      "Epoch 43, duration: 0.32, loss: 0.185767\n",
      "Accuracy of the network on the test dataset: 0.8460038986354775\n",
      "Epoch 44, duration: 0.32, loss: 0.187271\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 45, duration: 0.32, loss: 0.199285\n",
      "Accuracy of the network on the test dataset: 0.8752436647173489\n",
      "Epoch 46, duration: 0.32, loss: 0.208384\n",
      "Accuracy of the network on the test dataset: 0.8791423001949318\n",
      "Epoch 47, duration: 0.32, loss: 0.186352\n",
      "Accuracy of the network on the test dataset: 0.8635477582846004\n",
      "Epoch 48, duration: 0.32, loss: 0.187312\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 49, duration: 0.32, loss: 0.148663\n",
      "Accuracy of the network on the test dataset: 0.8576998050682261\n",
      "Epoch 50, duration: 0.32, loss: 0.128235\n",
      "Accuracy of the network on the test dataset: 0.9142300194931774\n",
      "Epoch 51, duration: 0.32, loss: 0.126234\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 52, duration: 0.32, loss: 0.183851\n",
      "Accuracy of the network on the test dataset: 0.8440545808966862\n",
      "Epoch 53, duration: 0.32, loss: 0.159207\n",
      "Accuracy of the network on the test dataset: 0.8810916179337231\n",
      "Epoch 54, duration: 0.32, loss: 0.190184\n",
      "Accuracy of the network on the test dataset: 0.8615984405458089\n",
      "Epoch 55, duration: 0.32, loss: 0.208102\n",
      "Accuracy of the network on the test dataset: 0.8518518518518519\n",
      "Epoch 56, duration: 0.32, loss: 0.159931\n",
      "Accuracy of the network on the test dataset: 0.8654970760233918\n",
      "Epoch 57, duration: 0.32, loss: 0.114867\n",
      "Accuracy of the network on the test dataset: 0.8927875243664717\n",
      "Epoch 58, duration: 0.32, loss: 0.103666\n",
      "Accuracy of the network on the test dataset: 0.8947368421052632\n",
      "Epoch 59, duration: 0.32, loss: 0.091325\n",
      "Accuracy of the network on the test dataset: 0.898635477582846\n",
      "Epoch 60, duration: 0.32, loss: 0.088061\n",
      "Accuracy of the network on the test dataset: 0.9044834307992202\n",
      "Epoch 61, duration: 0.32, loss: 0.065713\n",
      "Accuracy of the network on the test dataset: 0.9122807017543859\n",
      "Epoch 62, duration: 0.32, loss: 0.056016\n",
      "Accuracy of the network on the test dataset: 0.9083820662768031\n",
      "Epoch 63, duration: 0.32, loss: 0.044523\n",
      "Accuracy of the network on the test dataset: 0.8966861598440545\n",
      "Epoch 64, duration: 0.32, loss: 0.060833\n",
      "Accuracy of the network on the test dataset: 0.9220272904483431\n",
      "Epoch 65, duration: 0.32, loss: 0.068595\n",
      "Accuracy of the network on the test dataset: 0.9161793372319688\n",
      "Epoch 66, duration: 0.32, loss: 0.170414\n",
      "Accuracy of the network on the test dataset: 0.8615984405458089\n",
      "Epoch 67, duration: 0.32, loss: 0.157940\n",
      "Accuracy of the network on the test dataset: 0.8810916179337231\n",
      "Epoch 68, duration: 0.32, loss: 0.259093\n",
      "Accuracy of the network on the test dataset: 0.8752436647173489\n",
      "Epoch 69, duration: 0.32, loss: 0.151086\n",
      "Accuracy of the network on the test dataset: 0.8947368421052632\n",
      "Epoch 70, duration: 0.32, loss: 0.091147\n",
      "Accuracy of the network on the test dataset: 0.9064327485380117\n",
      "Epoch 71, duration: 0.32, loss: 0.062480\n",
      "Accuracy of the network on the test dataset: 0.9083820662768031\n",
      "Epoch 72, duration: 0.32, loss: 0.073687\n",
      "Accuracy of the network on the test dataset: 0.8771929824561403\n",
      "Epoch 73, duration: 0.32, loss: 0.105370\n",
      "Accuracy of the network on the test dataset: 0.9122807017543859\n",
      "Epoch 74, duration: 0.32, loss: 0.051380\n",
      "Accuracy of the network on the test dataset: 0.9220272904483431\n",
      "Epoch 75, duration: 0.32, loss: 0.058228\n",
      "Accuracy of the network on the test dataset: 0.9103313840155945\n",
      "Epoch 76, duration: 0.32, loss: 0.048359\n",
      "Accuracy of the network on the test dataset: 0.9103313840155945\n",
      "Epoch 77, duration: 0.32, loss: 0.065134\n",
      "Accuracy of the network on the test dataset: 0.898635477582846\n",
      "Epoch 78, duration: 0.32, loss: 0.041334\n",
      "Accuracy of the network on the test dataset: 0.9181286549707602\n",
      "Epoch 79, duration: 0.32, loss: 0.031127\n",
      "Accuracy of the network on the test dataset: 0.935672514619883\n",
      "Epoch 80, duration: 0.32, loss: 0.027305\n",
      "Accuracy of the network on the test dataset: 0.9200779727095516\n",
      "Epoch 81, duration: 0.32, loss: 0.022190\n",
      "Accuracy of the network on the test dataset: 0.898635477582846\n",
      "Epoch 82, duration: 0.32, loss: 0.017101\n",
      "Accuracy of the network on the test dataset: 0.9317738791423001\n",
      "Epoch 83, duration: 0.32, loss: 0.013029\n",
      "Accuracy of the network on the test dataset: 0.9278752436647173\n",
      "Epoch 84, duration: 0.32, loss: 0.010579\n",
      "Accuracy of the network on the test dataset: 0.935672514619883\n",
      "Epoch 85, duration: 0.32, loss: 0.016108\n",
      "Accuracy of the network on the test dataset: 0.9317738791423001\n",
      "Epoch 86, duration: 0.32, loss: 0.051258\n",
      "Accuracy of the network on the test dataset: 0.8810916179337231\n",
      "Epoch 87, duration: 0.32, loss: 0.047368\n",
      "Accuracy of the network on the test dataset: 0.9083820662768031\n",
      "Epoch 88, duration: 0.32, loss: 0.028930\n",
      "Accuracy of the network on the test dataset: 0.9220272904483431\n",
      "Epoch 89, duration: 0.32, loss: 0.015258\n",
      "Accuracy of the network on the test dataset: 0.9278752436647173\n",
      "Epoch 90, duration: 0.32, loss: 0.012180\n",
      "Accuracy of the network on the test dataset: 0.9337231968810916\n",
      "Epoch 91, duration: 0.32, loss: 0.011090\n",
      "Accuracy of the network on the test dataset: 0.9239766081871345\n",
      "Epoch 92, duration: 0.32, loss: 0.007816\n",
      "Accuracy of the network on the test dataset: 0.9395711500974658\n",
      "Epoch 93, duration: 0.32, loss: 0.011334\n",
      "Accuracy of the network on the test dataset: 0.9278752436647173\n",
      "Epoch 94, duration: 0.32, loss: 0.051904\n",
      "Accuracy of the network on the test dataset: 0.9142300194931774\n",
      "Epoch 95, duration: 0.32, loss: 0.122345\n",
      "Accuracy of the network on the test dataset: 0.9044834307992202\n",
      "Epoch 96, duration: 0.32, loss: 0.079121\n",
      "Accuracy of the network on the test dataset: 0.9025341130604289\n",
      "Epoch 97, duration: 0.32, loss: 0.082669\n",
      "Accuracy of the network on the test dataset: 0.8771929824561403\n",
      "Epoch 98, duration: 0.32, loss: 0.100331\n",
      "Accuracy of the network on the test dataset: 0.8888888888888888\n",
      "Epoch 99, duration: 0.32, loss: 0.054066\n",
      "Accuracy of the network on the test dataset: 0.9142300194931774\n",
      "Epoch 100, duration: 0.32, loss: 0.024740\n",
      "Accuracy of the network on the test dataset: 0.9317738791423001\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model_ft, training_losses = train(model_ft, criterion, optimizer, lrscheduler, n_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37265005-944e-4cd1-8174-d703595c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# model = WaveLength(c_in=411, c_out=6, nf=[47, 128, 128, 47])\n",
    "model = torch.load('weights/V1_6_multiClass.pth')\n",
    "model.eval()  # 切换模型到评估模式\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e96025f9-df23-43d6-9787-389d9719dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.9436\n",
      "f1_marco: 0.9439\n",
      "p_marco: 0.9432\n",
      "r_marco: 0.9452\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, label) in enumerate(test_loader, 0):\n",
    "        # images = images.to(device).half() # uncomment for half precision model\n",
    "        inputs = inputs.cuda()\n",
    "        label = label.cuda()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        test_labels.extend([label.cpu().numpy()])\n",
    "        test_predictions.extend([torch.sigmoid(outputs).cpu().numpy()])\n",
    "\n",
    "test_labels = np.vstack(test_labels)\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "test_predictions_prob = np.exp(test_predictions)/np.sum(np.exp(test_predictions),axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "test_predictions = np.argmax(test_predictions,axis=1)\n",
    "test_labels = np.argmax(test_labels,axis=1)\n",
    "\n",
    "avg_score = accuracy_score(test_labels, test_predictions)\n",
    "p_marco = precision_score(test_labels,test_predictions,average='macro')\n",
    "r_marco = recall_score(test_labels,test_predictions,average='macro')\n",
    "f1_marco = f1_score(test_labels,test_predictions,average='macro')\n",
    "print(\"Average accuracy: {:.4f}\".format(avg_score))\n",
    "print(\"f1_marco: {:.4f}\".format(f1_marco))\n",
    "print(\"p_marco: {:.4f}\".format(p_marco))\n",
    "print(\"r_marco: {:.4f}\".format(r_marco))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b467011f-2f29-4e2f-a052-14e1d3d73583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       138\n",
      "           1       0.94      0.97      0.96       105\n",
      "           2       0.93      0.88      0.90       138\n",
      "           3       0.92      0.94      0.93       133\n",
      "\n",
      "    accuracy                           0.94       514\n",
      "   macro avg       0.94      0.95      0.94       514\n",
      "weighted avg       0.94      0.94      0.94       514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_labels, test_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcef2d81-562e-4770-b29f-abff9f8f3b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 3, 1, 2, 2, 0, 2, 0, 2, 2, 0,\n",
       "       2, 1, 2, 3, 0, 2, 3, 1, 0, 2, 0, 3, 2, 0, 1, 1, 2, 3, 0, 3, 2, 1,\n",
       "       0, 3, 3, 3, 3, 0, 0, 3, 3, 0, 3, 1, 2, 2, 1, 1, 3, 1, 2, 3, 1, 2,\n",
       "       1, 2, 2, 0, 2, 0, 0, 3, 0, 2, 2, 0, 2, 1, 3, 2, 3, 0, 2, 0, 3, 1,\n",
       "       1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 2, 2, 2, 3,\n",
       "       1, 0, 1, 0, 2, 0, 1, 0, 0, 2, 3, 2, 3, 0, 2, 3, 0, 2, 1, 0, 2, 3,\n",
       "       1, 0, 3, 3, 2, 2, 0, 1, 2, 2, 3, 2, 3, 3, 2, 2, 0, 3, 3, 2, 0, 2,\n",
       "       3, 3, 1, 3, 2, 0, 1, 3, 2, 3, 0, 2, 0, 3, 0, 1, 1, 0, 0, 2, 1, 0,\n",
       "       2, 3, 1, 2, 0, 0, 2, 0, 3, 0, 3, 3, 0, 0, 2, 1, 0, 1, 3, 3, 2, 0,\n",
       "       0, 2, 0, 3, 3, 2, 1, 0, 1, 0, 1, 3, 2, 2, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       2, 1, 0, 2, 3, 0, 1, 3, 1, 3, 3, 0, 0, 0, 3, 0, 0, 1, 0, 0, 3, 2,\n",
       "       0, 2, 2, 1, 2, 0, 3, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 3, 0,\n",
       "       2, 0, 2, 0, 3, 1, 3, 3, 1, 0, 3, 3, 3, 2, 0, 3, 3, 1, 2, 2, 0, 0,\n",
       "       3, 3, 1, 3, 1, 0, 2, 0, 0, 3, 2, 0, 3, 3, 0, 2, 2, 0, 1, 3, 0, 3,\n",
       "       3, 0, 2, 0, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 1, 1, 3, 2, 2, 0, 3, 2,\n",
       "       0, 2, 3, 1, 2, 0, 3, 3, 1, 2, 1, 2, 2, 3, 2, 1, 0, 3, 3, 1, 1, 1,\n",
       "       2, 1, 3, 2, 0, 0, 2, 2, 3, 2, 0, 2, 1, 1, 3, 1, 0, 1, 3, 1, 0, 3,\n",
       "       1, 0, 1, 1, 3, 0, 3, 1, 3, 0, 0, 0, 3, 3, 2, 3, 1, 0, 3, 2, 0, 2,\n",
       "       2, 2, 2, 2, 1, 0, 0, 3, 1, 0, 0, 2, 3, 0, 0, 0, 3, 1, 0, 1, 3, 0,\n",
       "       3, 2, 3, 3, 0, 1, 0, 3, 0, 3, 0, 3, 3, 2, 3, 0, 2, 0, 2, 3, 1, 0,\n",
       "       1, 0, 0, 1, 3, 2, 0, 0, 1, 2, 0, 0, 3, 0, 3, 2, 3, 1, 0, 0, 3, 1,\n",
       "       0, 1, 0, 2, 0, 3, 0, 2, 1, 1, 2, 3, 3, 1, 0, 3, 3, 2, 3, 1, 3, 2,\n",
       "       0, 0, 1, 1, 1, 0, 2, 0, 2, 0, 3, 0, 2, 0, 0, 2, 1, 3, 1, 0, 1, 3,\n",
       "       0, 3, 0, 3, 2, 1, 0, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afdf1557-f0f4-448e-a31f-f24f38d078fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 1, 2, 1, 3, 1, 3, 2, 2, 0, 3, 1, 2, 2, 0, 2, 0, 2, 2, 0,\n",
       "       2, 1, 1, 3, 0, 2, 3, 1, 0, 2, 0, 2, 2, 0, 1, 1, 2, 3, 0, 3, 2, 1,\n",
       "       0, 2, 3, 3, 3, 0, 0, 2, 2, 1, 2, 1, 2, 2, 1, 1, 3, 1, 2, 3, 3, 2,\n",
       "       1, 2, 2, 0, 2, 0, 0, 3, 0, 2, 2, 0, 2, 1, 3, 2, 3, 0, 2, 0, 3, 3,\n",
       "       1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 2, 2, 0, 1, 1, 1, 0, 1, 2, 2, 2, 3,\n",
       "       1, 0, 2, 0, 2, 0, 1, 0, 0, 2, 3, 2, 3, 0, 2, 3, 0, 2, 1, 0, 2, 3,\n",
       "       1, 0, 3, 3, 3, 2, 0, 1, 3, 2, 3, 2, 3, 3, 2, 2, 0, 2, 3, 2, 0, 2,\n",
       "       3, 3, 1, 3, 2, 1, 1, 2, 2, 3, 0, 3, 0, 3, 0, 1, 1, 0, 0, 2, 1, 0,\n",
       "       2, 3, 1, 2, 0, 0, 2, 0, 3, 0, 2, 3, 0, 0, 2, 1, 0, 1, 3, 3, 2, 0,\n",
       "       0, 2, 0, 3, 3, 2, 2, 0, 1, 0, 1, 3, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       2, 1, 0, 2, 3, 0, 1, 3, 1, 3, 3, 0, 0, 0, 2, 0, 0, 1, 0, 0, 3, 2,\n",
       "       0, 2, 2, 1, 2, 0, 3, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1, 1, 2, 2, 3, 0,\n",
       "       3, 1, 3, 0, 3, 2, 3, 3, 1, 0, 3, 3, 2, 2, 0, 3, 2, 1, 2, 2, 0, 0,\n",
       "       3, 3, 1, 3, 1, 0, 2, 0, 0, 3, 2, 0, 3, 3, 0, 2, 2, 0, 1, 3, 1, 3,\n",
       "       3, 0, 2, 0, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 0, 3, 2,\n",
       "       0, 2, 3, 1, 2, 0, 3, 3, 1, 2, 1, 2, 2, 3, 2, 1, 0, 3, 3, 1, 1, 1,\n",
       "       2, 1, 3, 2, 0, 0, 2, 2, 3, 2, 0, 2, 1, 2, 3, 1, 0, 1, 3, 3, 0, 3,\n",
       "       1, 0, 1, 2, 3, 0, 3, 1, 3, 0, 0, 0, 3, 3, 2, 3, 1, 0, 3, 2, 0, 2,\n",
       "       2, 3, 2, 2, 1, 0, 0, 3, 1, 0, 0, 2, 3, 0, 0, 0, 3, 1, 0, 1, 3, 0,\n",
       "       3, 2, 3, 3, 1, 2, 0, 3, 0, 3, 0, 3, 3, 2, 3, 0, 2, 0, 2, 3, 3, 0,\n",
       "       1, 0, 0, 1, 3, 2, 0, 0, 1, 2, 1, 0, 3, 0, 3, 2, 3, 1, 0, 0, 3, 1,\n",
       "       0, 1, 0, 2, 0, 3, 0, 2, 1, 1, 2, 3, 3, 1, 0, 3, 3, 2, 2, 1, 3, 2,\n",
       "       0, 0, 1, 1, 1, 0, 2, 0, 2, 1, 3, 0, 2, 0, 0, 2, 1, 3, 1, 0, 1, 3,\n",
       "       0, 3, 0, 3, 2, 1, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de377f13-e81b-4ac0-8c25-401daee96202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1 1 2 2 3 3 4 4 5 5 0 0]\n",
    "# [1 0 2 2 3 5 5 5 5 5 0 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
